{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5pK5IOCebgH",
        "outputId": "38457a8d-4990-427f-d6a2-69b1cc920eee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Collecting spark-nlp==5.3.0\n",
            "  Downloading spark_nlp-5.3.0-py2.py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from flask) (1.9.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->flask) (3.0.2)\n",
            "Downloading spark_nlp-5.3.0-py2.py3-none-any.whl (564 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.8/564.8 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyngrok-7.2.8-py3-none-any.whl (25 kB)\n",
            "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: spark-nlp, pyngrok, faiss-cpu\n",
            "Successfully installed faiss-cpu-1.11.0 pyngrok-7.2.8 spark-nlp-5.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyngrok flask pyspark spark-nlp==5.3.0 faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_slhN0gooUCE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# use your own ngrok authtoken\n",
        "os.environ['NGROK_AUTHTOKEN'] = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oSKZ8UGPopf",
        "outputId": "7097d532-c5e0-4626-ae22-c5b6de369529"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Google Drive mounted successfully.\n",
            "Initializing Spark session...\n",
            "Spark session started.\n",
            "\n",
            "Preparing query embedding generator...\n",
            "sent_small_bert_L2_128 download started this may take some time.\n",
            "Approximate size to download 16.1 MB\n",
            "[OK!]\n",
            "Query embedding generator ready.\n",
            "\n",
            "Loading QA metadata from: /content/drive/My Drive/QA_dataset/qa_combined_embeddings.parquet\n",
            "Loading YT metadata from: /content/drive/My Drive/Youtube_100M_dataset_v3/combined_video_embeddings.parquet\n",
            "Temporarily disabled Arrow for QA metadata toPandas.\n",
            "QA metadata loaded. Records: 680495\n",
            "Restored Arrow to true for QA.\n",
            "Temporarily disabled Arrow for YT metadata toPandas.\n",
            "YT metadata loaded. Records: 22222\n",
            "Restored Arrow to true for YT.\n",
            "\n",
            "Loading QA FAISS index from: /content/drive/My Drive/QA_dataset/qa_combined_embeddings.index\n",
            "Loading YT FAISS index from: /content/drive/My Drive/Youtube_100M_dataset_v3/combined_video_embeddings.index\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "QA FAISS index loaded. Vectors: 680495\n",
            "YT FAISS index loaded. Vectors: 22222\n",
            "\\n--- Loading CLIP Model and Video_embed Resources ---\n",
            "Using device for CLIP model: cpu\n",
            "Loading CLIP processor: openai/clip-vit-base-patch16...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading CLIP model: openai/clip-vit-base-patch16...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-05-12T03:39:44+0000 lvl=warn msg=\"can't bind default web address, trying alternatives\" obj=web addr=127.0.0.1:4040\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CLIP model and processor loaded successfully.\n",
            "Loading CLIP FAISS index from: /content/drive/MyDrive/video_embeddings_project/video_embeddings.index...\n",
            "CLIP FAISS index loaded. Vectors: 21300\n",
            "Loading CLIP metadata from: /content/drive/MyDrive/video_embeddings_project/metadata.pkl...\n",
            "CLIP metadata loaded. Entries: 21610\n",
            "CLIP resources loaded successfully.\n",
            "\n",
            "--- Setting up ngrok tunnel ---\n",
            "NGROK Authtoken set.\n",
            " * ngrok tunnel \"https://cs6513edu.ngrok.app\" -> \"http://127.0.0.1:5000\"\n",
            " * Access the service at: https://cs6513edu.ngrok.app\n",
            " * Example search: https://cs6513edu.ngrok.app/search?query=your_query_here&k=3\n",
            "\n",
            "--- Starting Flask App in a background thread ---\n",
            "Flask app is running. Colab cell will remain active.\n",
            "To stop: interrupt/stop the Colab kernel.\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: off\n",
            "Received search request: Query='how to bake cake', k=3\n",
            "Attempting CLIP reranking for YouTube results (initial count: 3).\n",
            "Reranking 3 YouTube results with CLIP for query: 'how to bake cake'.\n",
            "Searching CLIP FAISS index (top 500 frames)...\n",
            "Found CLIP scores for 168 unique video IDs.\n",
            "CLIP reranking process completed.\n",
            "Search for 'how to bake cake' (k=3) completed. Found 3 QA results, 3 YT results.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import faiss\n",
        "import pandas as pd\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "from sparknlp.base import DocumentAssembler\n",
        "from sparknlp.annotator import BertSentenceEmbeddings\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "# --- Flask and ngrok imports (from service script) ---\n",
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "import threading\n",
        "import logging\n",
        "\n",
        "# --- Additional imports for CLIP model and Video_embed integration ---\n",
        "import torch\n",
        "import pickle\n",
        "from transformers import CLIPModel, CLIPProcessor\n",
        "\n",
        "# Suppress excessive pyngrok logging\n",
        "pyngrok_logger = logging.getLogger(\"pyngrok\")\n",
        "pyngrok_logger.setLevel(logging.WARNING)\n",
        "\n",
        "# --- Mount Google Drive --- (Moved here for early execution if needed by Flask later)\n",
        "from google.colab import drive\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}\")\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# QA Data Paths (from build_qa_faiss.py)\n",
        "QA_BASE_PATH = '/content/drive/My Drive/QA_dataset'\n",
        "EMBEDDINGS_PARQUET_PATH_QA = os.path.join(QA_BASE_PATH, \"qa_combined_embeddings.parquet\")\n",
        "FAISS_INDEX_PATH_QA = os.path.join(QA_BASE_PATH, \"qa_combined_embeddings.index\")\n",
        "\n",
        "# YouTube transcripts and titles embeddings Data Paths (from build_youtube_faiss.py)\n",
        "YT_BASE_PATH = '/content/drive/My Drive/Youtube_100M_dataset_v3'\n",
        "EMBEDDINGS_PARQUET_PATH_YT = os.path.join(YT_BASE_PATH, \"combined_video_embeddings.parquet\")\n",
        "FAISS_INDEX_PATH_YT = os.path.join(YT_BASE_PATH, \"combined_video_embeddings.index\")\n",
        "\n",
        "# CLIP Model and Video_embed Data Paths\n",
        "CLIP_PROJECT_DIR = '/content/drive/MyDrive/video_embeddings_project' # Base directory from Video_embed.ipynb\n",
        "CLIP_FAISS_INDEX_PATH = os.path.join(CLIP_PROJECT_DIR, \"video_embeddings.index\")\n",
        "CLIP_METADATA_PATH = os.path.join(CLIP_PROJECT_DIR, \"metadata.pkl\")\n",
        "\n",
        "# New Educational Video Paths\n",
        "CLIP_EDU_PROJECT_DIR = '/content/drive/MyDrive/video_embeddings_project_edu'\n",
        "CLIP_EDU_FAISS_INDEX_PATH = os.path.join(CLIP_EDU_PROJECT_DIR, \"video_embeddings.index\")\n",
        "CLIP_EDU_METADATA_PATH = os.path.join(CLIP_EDU_PROJECT_DIR, \"metadata.pkl\")\n",
        "\n",
        "CLIP_MODEL_NAME = \"openai/clip-vit-base-patch16\"\n",
        "\n",
        "# --- Global variables for service (expanded from service script) ---\n",
        "spark = None\n",
        "query_embedding_pipeline_model = None\n",
        "faiss_index_qa = None\n",
        "metadata_pd_df_qa = None\n",
        "faiss_index_yt = None\n",
        "metadata_pd_df_yt = None\n",
        "ngrok_public_url = None\n",
        "\n",
        "# Global variables for CLIP integration\n",
        "clip_model = None\n",
        "clip_processor = None\n",
        "faiss_index_clip = None\n",
        "metadata_clip = None # This will be a list of dicts\n",
        "clip_device = None\n",
        "\n",
        "# --- Helper functions for merging FAISS indices and metadata ---\n",
        "def merge_and_deduplicate_faiss_indices(existing_index, new_index):\n",
        "    \"\"\"\n",
        "    Merge two FAISS indices and remove duplicate vectors, using batch processing for better performance\n",
        "    \"\"\"\n",
        "    if existing_index is None:\n",
        "        return new_index, range(new_index.ntotal)\n",
        "\n",
        "    if new_index is None:\n",
        "        return existing_index, range(existing_index.ntotal)\n",
        "\n",
        "    print(\"Starting to merge and deduplicate FAISS indices...\")\n",
        "\n",
        "    # Ensure they have the same dimensions\n",
        "    assert existing_index.d == new_index.d, \"The two indices have mismatched vector dimensions\"\n",
        "\n",
        "    # Create merged index\n",
        "    merged_index = faiss.IndexFlatIP(existing_index.d)\n",
        "\n",
        "    # Add all vectors from the existing index\n",
        "    existing_vectors = np.zeros((existing_index.ntotal, existing_index.d), dtype=np.float32)\n",
        "    for i in range(existing_index.ntotal):\n",
        "        vector = faiss.vector_to_array(existing_index.get_vector(i)).reshape(1, -1)\n",
        "        existing_vectors[i] = vector\n",
        "\n",
        "    merged_index.add(existing_vectors)\n",
        "    print(f\"Added {existing_index.ntotal} vectors from existing index\")\n",
        "\n",
        "    # Create a mapping for valid indices of new vectors\n",
        "    valid_indices = []\n",
        "\n",
        "    # Process new index vectors in batches instead of one at a time\n",
        "    batch_size = 100  # Number of vectors to process per batch\n",
        "    total_new_vectors = new_index.ntotal\n",
        "    total_batches = (total_new_vectors + batch_size - 1) // batch_size  # Ceiling division\n",
        "\n",
        "    total_added = 0\n",
        "\n",
        "    for batch_idx in range(total_batches):\n",
        "        start_idx = batch_idx * batch_size\n",
        "        end_idx = min(start_idx + batch_size, total_new_vectors)\n",
        "        batch_size_actual = end_idx - start_idx\n",
        "\n",
        "        # Extract vectors for this batch\n",
        "        batch_vectors = np.zeros((batch_size_actual, new_index.d), dtype=np.float32)\n",
        "        for i in range(batch_size_actual):\n",
        "            vector_idx = start_idx + i\n",
        "            vector = faiss.vector_to_array(new_index.get_vector(vector_idx)).reshape(1, -1)\n",
        "            batch_vectors[i] = vector\n",
        "\n",
        "        # Perform search on the entire batch of vectors\n",
        "        distances, _ = merged_index.search(batch_vectors, 1)\n",
        "\n",
        "        # Process results and add non-duplicate vectors\n",
        "        new_vectors_to_add = []\n",
        "        for i in range(batch_size_actual):\n",
        "            if distances[i][0] < 0.99:  # If similarity is below threshold, consider it a new vector\n",
        "                new_vectors_to_add.append(batch_vectors[i].reshape(1, -1))\n",
        "                valid_indices.append(start_idx + i)\n",
        "\n",
        "        # Add non-duplicate vectors in batch\n",
        "        if new_vectors_to_add:\n",
        "            new_vectors_array = np.vstack(new_vectors_to_add)\n",
        "            merged_index.add(new_vectors_array)\n",
        "            total_added += len(new_vectors_to_add)\n",
        "\n",
        "        # Print progress\n",
        "        if (batch_idx + 1) % 10 == 0 or batch_idx == total_batches - 1:\n",
        "            progress = (batch_idx + 1) / total_batches * 100\n",
        "            print(f\"Processing progress: {progress:.1f}% ({batch_idx + 1}/{total_batches} batches), added {total_added} non-duplicate vectors\")\n",
        "\n",
        "    print(f\"Added {total_added} non-duplicate vectors from new index\")\n",
        "    print(f\"Merged index now contains {merged_index.ntotal} vectors total\")\n",
        "\n",
        "    return merged_index, valid_indices\n",
        "\n",
        "def merge_and_deduplicate_metadata(existing_metadata, new_metadata, valid_indices):\n",
        "    \"\"\"\n",
        "    Merge metadata and keep only the new metadata corresponding to valid indices\n",
        "    \"\"\"\n",
        "    if existing_metadata is None:\n",
        "        if new_metadata is None:\n",
        "            return []\n",
        "        return [new_metadata[i] for i in valid_indices]\n",
        "\n",
        "    if new_metadata is None:\n",
        "        return existing_metadata\n",
        "\n",
        "    # Only add new metadata corresponding to valid indices\n",
        "    filtered_new_metadata = [new_metadata[i] for i in valid_indices]\n",
        "\n",
        "    # Check for duplicate video IDs\n",
        "    existing_video_ids = set(item.get('video_id', '') for item in existing_metadata if item.get('video_id'))\n",
        "\n",
        "    # Remove metadata with duplicate video IDs\n",
        "    unique_new_metadata = []\n",
        "    duplicate_count = 0\n",
        "\n",
        "    for item in filtered_new_metadata:\n",
        "        video_id = item.get('video_id', '')\n",
        "        if video_id and video_id in existing_video_ids:\n",
        "            duplicate_count += 1\n",
        "            continue\n",
        "        unique_new_metadata.append(item)\n",
        "        if video_id:\n",
        "            existing_video_ids.add(video_id)\n",
        "\n",
        "    print(f\"Metadata deduplication: Ignored {duplicate_count} entries with duplicate video IDs\")\n",
        "\n",
        "    merged_metadata = existing_metadata + unique_new_metadata\n",
        "    print(f\"Merged metadata now contains {len(merged_metadata)} entries total\")\n",
        "\n",
        "    return merged_metadata\n",
        "\n",
        "# --- INITIALIZATION (Functions from build_qa_faiss_version2.py, called at startup) ---\n",
        "print(\"Initializing Spark session...\")\n",
        "try:\n",
        "    spark = SparkSession.builder \\\n",
        "        .appName(\"Combined_FAISS_Service_Spark\") \\\n",
        "        .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.3.0\") \\\n",
        "        .config(\"spark.driver.memory\", \"16G\") \\\n",
        "        .config(\"spark.driver.maxResultSize\", \"8g\") \\\n",
        "        .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
        "        .master(\"local[*]\") \\\n",
        "        .getOrCreate()\n",
        "    print(\"Spark session started.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error initializing Spark session: {e}\")\n",
        "    exit() # Critical failure\n",
        "\n",
        "def build_query_pipeline_global(): # Renamed to avoid conflict if other local funcs exist\n",
        "    document_query_template = DocumentAssembler().setInputCol(\"text\").setOutputCol(\"document\")\n",
        "    embeddings_model_template = BertSentenceEmbeddings.pretrained(\"sent_small_bert_L2_128\") \\\n",
        "        .setInputCols([\"document\"]).setOutputCol(\"embedding\").setCaseSensitive(False)\n",
        "    query_pipeline_template = Pipeline(stages=[document_query_template, embeddings_model_template])\n",
        "    dummy_df_for_fit = spark.createDataFrame([(\"dummy query text\",)], [\"text\"])\n",
        "    return query_pipeline_template.fit(dummy_df_for_fit)\n",
        "\n",
        "print(\"\\nPreparing query embedding generator...\")\n",
        "try:\n",
        "    query_embedding_pipeline_model = build_query_pipeline_global()\n",
        "    print(\"Query embedding generator ready.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: Could not prepare query embedding generator: {e}\")\n",
        "    if spark: spark.stop()\n",
        "    exit() # Critical failure\n",
        "\n",
        "def load_metadata_global():\n",
        "    global metadata_pd_df_qa, metadata_pd_df_yt # Explicitly declare we are modifying globals\n",
        "    print(f\"\\nLoading QA metadata from: {EMBEDDINGS_PARQUET_PATH_QA}\")\n",
        "    print(f\"Loading YT metadata from: {EMBEDDINGS_PARQUET_PATH_YT}\")\n",
        "    current_arrow_status_qa = spark.conf.get(\"spark.sql.execution.arrow.pyspark.enabled\")\n",
        "    current_arrow_status_yt = spark.conf.get(\"spark.sql.execution.arrow.pyspark.enabled\") # Redundant but for clarity per df\n",
        "    try:\n",
        "        spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
        "        print(\"Temporarily disabled Arrow for QA metadata toPandas.\")\n",
        "        loaded_metadata_spark_df_qa = spark.read.parquet(EMBEDDINGS_PARQUET_PATH_QA)\n",
        "        metadata_pd_df_qa = loaded_metadata_spark_df_qa.select(\"qa_id\", \"Question\", \"Answer\", \"Level\").toPandas()\n",
        "        print(f\"QA metadata loaded. Records: {len(metadata_pd_df_qa)}\")\n",
        "        spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", current_arrow_status_qa)\n",
        "        print(f\"Restored Arrow to {current_arrow_status_qa} for QA.\")\n",
        "\n",
        "        spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
        "        print(\"Temporarily disabled Arrow for YT metadata toPandas.\")\n",
        "        loaded_metadata_spark_df_yt = spark.read.parquet(EMBEDDINGS_PARQUET_PATH_YT)\n",
        "        metadata_pd_df_yt = loaded_metadata_spark_df_yt.select(\"video_id\", \"title\", \"transcript\").toPandas()\n",
        "        print(f\"YT metadata loaded. Records: {len(metadata_pd_df_yt)}\")\n",
        "        spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", current_arrow_status_yt)\n",
        "        print(f\"Restored Arrow to {current_arrow_status_yt} for YT.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: Could not load or process Parquet metadata. Error: {e}\")\n",
        "        # Restore arrow status in case of failure during one of the loads\n",
        "        if spark:\n",
        "            try:\n",
        "                spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") # Best guess restore\n",
        "                print(\"Attempted to restore Arrow to true after metadata load error.\")\n",
        "            except Exception as conf_e:\n",
        "                 print(f\"Could not restore Arrow config after error: {conf_e}\")\n",
        "        return False # Indicate failure\n",
        "    return True # Indicate success\n",
        "\n",
        "if not load_metadata_global():\n",
        "    print(\"Critical Error: Metadata loading failed. Service cannot start.\")\n",
        "    if spark: spark.stop()\n",
        "    exit()\n",
        "\n",
        "def load_faiss_indices_global():\n",
        "    global faiss_index_qa, faiss_index_yt # Explicitly declare we are modifying globals\n",
        "    print(f\"\\nLoading QA FAISS index from: {FAISS_INDEX_PATH_QA}\")\n",
        "    print(f\"Loading YT FAISS index from: {FAISS_INDEX_PATH_YT}\")\n",
        "    try:\n",
        "        faiss_index_qa = faiss.read_index(FAISS_INDEX_PATH_QA)\n",
        "        print(f\"QA FAISS index loaded. Vectors: {faiss_index_qa.ntotal}\")\n",
        "        faiss_index_yt = faiss.read_index(FAISS_INDEX_PATH_YT)\n",
        "        print(f\"YT FAISS index loaded. Vectors: {faiss_index_yt.ntotal}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: Could not load FAISS index files. Error: {e}\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "if not load_faiss_indices_global():\n",
        "    print(\"Critical Error: FAISS index loading failed. Service cannot start.\")\n",
        "    if spark: spark.stop()\n",
        "    exit()\n",
        "\n",
        "def load_clip_resources():\n",
        "    \"\"\"Loads the CLIP model, processor, FAISS index, and metadata from Video_embed.ipynb.\"\"\"\n",
        "    global clip_model, clip_processor, faiss_index_clip, metadata_clip, clip_device\n",
        "\n",
        "    print(\"\\\\n--- Loading CLIP Model and Video_embed Resources ---\")\n",
        "    try:\n",
        "        clip_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Using device for CLIP model: {clip_device}\")\n",
        "\n",
        "        print(f\"Loading CLIP processor: {CLIP_MODEL_NAME}...\")\n",
        "        clip_processor = CLIPProcessor.from_pretrained(CLIP_MODEL_NAME)\n",
        "        print(f\"Loading CLIP model: {CLIP_MODEL_NAME}...\")\n",
        "        clip_model = CLIPModel.from_pretrained(CLIP_MODEL_NAME).to(clip_device)\n",
        "        clip_model.eval() # Set to evaluation mode\n",
        "        print(\"CLIP model and processor loaded successfully.\")\n",
        "\n",
        "        # 1. Load original FAISS index and metadata\n",
        "        original_faiss_index = None\n",
        "        original_metadata = None\n",
        "\n",
        "        print(f\"Loading original CLIP FAISS index from: {CLIP_FAISS_INDEX_PATH}...\")\n",
        "        if os.path.exists(CLIP_FAISS_INDEX_PATH):\n",
        "            original_faiss_index = faiss.read_index(CLIP_FAISS_INDEX_PATH)\n",
        "            print(f\"Original CLIP FAISS index loaded. Vector count: {original_faiss_index.ntotal}\")\n",
        "        else:\n",
        "            print(f\"Warning: Original CLIP FAISS index not found at {CLIP_FAISS_INDEX_PATH}\")\n",
        "\n",
        "        print(f\"Loading original CLIP metadata from: {CLIP_METADATA_PATH}...\")\n",
        "        if os.path.exists(CLIP_METADATA_PATH):\n",
        "            with open(CLIP_METADATA_PATH, 'rb') as f:\n",
        "                original_metadata = pickle.load(f)\n",
        "            print(f\"Original CLIP metadata loaded. Entry count: {len(original_metadata)}\")\n",
        "            # Basic validation of metadata structure\n",
        "            if not isinstance(original_metadata, list) or (len(original_metadata) > 0 and not isinstance(original_metadata[0], dict)):\n",
        "                print(\"Warning: Original CLIP metadata is not a list of dictionaries\")\n",
        "        else:\n",
        "            print(f\"Warning: Original CLIP metadata not found at {CLIP_METADATA_PATH}\")\n",
        "\n",
        "        # 2. Load educational video FAISS index and metadata\n",
        "        edu_faiss_index = None\n",
        "        edu_metadata = None\n",
        "\n",
        "        print(f\"Loading educational CLIP FAISS index from: {CLIP_EDU_FAISS_INDEX_PATH}...\")\n",
        "        if os.path.exists(CLIP_EDU_FAISS_INDEX_PATH):\n",
        "            edu_faiss_index = faiss.read_index(CLIP_EDU_FAISS_INDEX_PATH)\n",
        "            print(f\"Educational CLIP FAISS index loaded. Vector count: {edu_faiss_index.ntotal}\")\n",
        "        else:\n",
        "            print(f\"Warning: Educational CLIP FAISS index not found at {CLIP_EDU_FAISS_INDEX_PATH}\")\n",
        "\n",
        "        print(f\"Loading educational CLIP metadata from: {CLIP_EDU_METADATA_PATH}...\")\n",
        "        if os.path.exists(CLIP_EDU_METADATA_PATH):\n",
        "            with open(CLIP_EDU_METADATA_PATH, 'rb') as f:\n",
        "                edu_metadata = pickle.load(f)\n",
        "            print(f\"Educational CLIP metadata loaded. Entry count: {len(edu_metadata)}\")\n",
        "            # Basic validation of metadata structure\n",
        "            if not isinstance(edu_metadata, list) or (len(edu_metadata) > 0 and not isinstance(edu_metadata[0], dict)):\n",
        "                print(\"Warning: Educational CLIP metadata is not a list of dictionaries\")\n",
        "        else:\n",
        "            print(f\"Warning: Educational CLIP metadata not found at {CLIP_EDU_METADATA_PATH}\")\n",
        "\n",
        "        # 3. Merge indices and metadata (with deduplication)\n",
        "        if original_faiss_index is None and edu_faiss_index is None:\n",
        "            print(\"Error: All CLIP FAISS indices failed to load\")\n",
        "            return False\n",
        "\n",
        "        # If only one index is available, use it directly\n",
        "        if original_faiss_index is None:\n",
        "            faiss_index_clip = edu_faiss_index\n",
        "            metadata_clip = edu_metadata\n",
        "            print(\"Using only educational video index and metadata\")\n",
        "        elif edu_faiss_index is None:\n",
        "            faiss_index_clip = original_faiss_index\n",
        "            metadata_clip = original_metadata\n",
        "            print(\"Using only original index and metadata\")\n",
        "        else:\n",
        "            # Merge and deduplicate both indices and metadata\n",
        "            print(\"Merging both indices and metadata with deduplication...\")\n",
        "            merged_index, valid_indices = merge_and_deduplicate_faiss_indices(original_faiss_index, edu_faiss_index)\n",
        "            merged_metadata = merge_and_deduplicate_metadata(original_metadata, edu_metadata, valid_indices)\n",
        "\n",
        "            faiss_index_clip = merged_index\n",
        "            metadata_clip = merged_metadata\n",
        "            print(\"Completed merging and deduplication of indices and metadata!\")\n",
        "\n",
        "        print(f\"Final FAISS index contains {faiss_index_clip.ntotal} vectors\")\n",
        "        print(f\"Final metadata contains {len(metadata_clip)} entries\")\n",
        "\n",
        "        print(\"CLIP resources loaded successfully.\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading CLIP resources: {e}\")\n",
        "        # import traceback\n",
        "        # traceback.print_exc()\n",
        "        return False\n",
        "\n",
        "# Attempt to load CLIP resources for reranking\n",
        "if not load_clip_resources():\n",
        "    print(\"Warning: CLIP resources for reranking could not be loaded. Reranking will be disabled.\")\n",
        "    # Depending on requirements, you might choose to exit() if CLIP is critical\n",
        "    # For now, we'll allow the service to run without CLIP reranking capabilities.\n",
        "    faiss_index_clip = None # Ensure it's None if loading failed\n",
        "    metadata_clip = None\n",
        "\n",
        "# Critical checks after loading (ensure all necessary components are ready)\n",
        "components_ready = [\n",
        "    spark is not None,\n",
        "    query_embedding_pipeline_model is not None,\n",
        "    faiss_index_qa is not None,\n",
        "    metadata_pd_df_qa is not None, # Checks if it's assigned (not None)\n",
        "    faiss_index_yt is not None,\n",
        "    metadata_pd_df_yt is not None,  # Checks if it's assigned (not None)\n",
        "    # CLIP components are optional for base functionality, but checked here for completeness if loaded\n",
        "    # If load_clip_resources() returned False, these will be None and reranking won't occur.\n",
        "]\n",
        "\n",
        "if not all(components_ready):\n",
        "    print(\"Critical Error: One or more essential components failed to initialize. Service cannot start.\")\n",
        "    if spark and spark.getActiveSession(): spark.stop()\n",
        "    exit()\n",
        "\n",
        "# --- EMBEDDING FUNCTION (from build_qa_faiss_version2.py) ---\n",
        "def embed_query(query_text, pipeline_model, spark_session):\n",
        "    if pipeline_model is None:\n",
        "        # This should be caught by startup checks, but good for robustness\n",
        "        raise RuntimeError(\"Query embedding pipeline model is not initialized.\")\n",
        "    query_df = spark_session.createDataFrame([(\"query_id_temp\", query_text)], [\"id\", \"text\"])\n",
        "    query_embedded_df = pipeline_model.transform(query_df)\n",
        "    embedding_vector = query_embedded_df.select(F.col(\"embedding.embeddings\")[0].alias(\"vector\")).collect()[0][\"vector\"]\n",
        "    return np.array([embedding_vector]).astype('float32')\n",
        "\n",
        "def def_query_clip(query_text, model, processor, device):\n",
        "    \"\"\"Generates CLIP embedding for a given text query.\"\"\"\n",
        "    if model is None or processor is None:\n",
        "        # This should be caught by startup checks if CLIP resources are critical,\n",
        "        # or handled gracefully if reranking is optional.\n",
        "        raise RuntimeError(\"CLIP model or processor is not initialized.\")\n",
        "    try:\n",
        "        inputs = processor(text=[query_text], return_tensors=\"pt\", padding=True).to(device)\n",
        "        with torch.no_grad():\n",
        "            text_embedding = model.get_text_features(**inputs)\n",
        "        text_embedding = text_embedding / text_embedding.norm(dim=-1, keepdim=True)\n",
        "        return text_embedding.cpu().numpy().astype('float32')\n",
        "    except Exception as e:\n",
        "        print(f\"Error generating CLIP embedding for query '{query_text}': {e}\")\n",
        "        # import traceback\n",
        "        # traceback.print_exc()\n",
        "        return None # Or raise an error\n",
        "\n",
        "# --- SEARCH FUNCTIONS (from build_qa_faiss_version2.py, minor changes for JSON) ---\n",
        "# Note: These now return lists of dicts for easier jsonify, instead of DataFrames.\n",
        "# Also, added explicit type conversion for JSON serializability where pandas might use numpy types.\n",
        "\n",
        "def search_qa_faiss_service(index, query_embedding_np, k, metadata_df_ref):\n",
        "    if index is None or metadata_df_ref is None:\n",
        "        return [] # Return empty list if components missing\n",
        "    distances, indices = index.search(query_embedding_np, k)\n",
        "    results = []\n",
        "    for i, (idx_val, dist) in enumerate(zip(indices[0], distances[0])):\n",
        "        py_idx = int(idx_val)\n",
        "        similarity_score = 1 / (1 + dist) if dist >= 0 else float('inf')\n",
        "        item_data = {\"rank\": i + 1, \"distance\": f\"{dist:.4f}\", \"similarity\": f\"{similarity_score:.4f}\"}\n",
        "        if 0 <= py_idx < len(metadata_df_ref):\n",
        "            row_data = metadata_df_ref.iloc[py_idx]\n",
        "            qa_id_val = row_data.get(\"qa_id\")\n",
        "            item_data[\"qa_id\"] = int(qa_id_val) if isinstance(qa_id_val, np.integer) else str(qa_id_val if pd.notna(qa_id_val) else \"N/A\")\n",
        "            item_data[\"Question\"] = str(row_data.get(\"Question\", \"N/A\"))\n",
        "            item_data[\"Answer\"] = str(row_data.get(\"Answer\", \"N/A\"))\n",
        "            level_val = row_data.get(\"Level\")\n",
        "            item_data[\"Level\"] = int(level_val) if isinstance(level_val, np.integer) else str(level_val if pd.notna(level_val) else \"N/A\")\n",
        "        else:\n",
        "            item_data[\"internal_faiss_id\"] = py_idx\n",
        "            item_data[\"Error\"] = \"Metadata not found for this index\"\n",
        "        results.append(item_data)\n",
        "    return results\n",
        "\n",
        "def search_yt_faiss_service(index, query_embedding_np, k, metadata_df_ref):\n",
        "    if index is None or metadata_df_ref is None:\n",
        "        return []\n",
        "    distances, indices = index.search(query_embedding_np, k)\n",
        "    results = []\n",
        "    for i, (idx_val, dist) in enumerate(zip(indices[0], distances[0])):\n",
        "        py_idx = int(idx_val)\n",
        "        similarity_score = 1 / (1 + dist) if dist >= 0 else float('inf')\n",
        "        item_data = {\"rank\": i + 1, \"distance\": f\"{dist:.4f}\", \"similarity\": f\"{similarity_score:.4f}\"}\n",
        "        if 0 <= py_idx < len(metadata_df_ref):\n",
        "            row_data = metadata_df_ref.iloc[py_idx]\n",
        "            item_data[\"video_id\"] = str(row_data.get(\"video_id\", f\"InternalID-{py_idx}\"))\n",
        "            item_data[\"title\"] = str(row_data.get(\"title\", \"(No title information)\"))\n",
        "            item_data[\"transcript\"] = str(row_data.get(\"transcript\", \"(No transcript information)\"))\n",
        "        else:\n",
        "            item_data[\"internal_faiss_id\"] = py_idx\n",
        "            item_data[\"Error\"] = \"Metadata not found for this index\"\n",
        "        results.append(item_data)\n",
        "    return results\n",
        "\n",
        "def rerank_youtube_results_with_clip(initial_yt_results, query_text, k_for_rerank,\n",
        "                                     clip_m, clip_p, clip_idx, clip_meta, device):\n",
        "    \"\"\"Reranks YouTube search results using CLIP embeddings and FAISS index from Video_embed.ipynb.\"\"\"\n",
        "    if not clip_idx or not clip_meta or not clip_m or not clip_p:\n",
        "        print(\"CLIP resources not available, skipping reranking.\")\n",
        "        return initial_yt_results # Return original results if CLIP components are missing\n",
        "\n",
        "    print(f\"Reranking {len(initial_yt_results)} YouTube results with CLIP for query: '{query_text}'.\")\n",
        "\n",
        "    try:\n",
        "        clip_query_embedding = def_query_clip(query_text, clip_m, clip_p, device)\n",
        "        if clip_query_embedding is None:\n",
        "            print(\"Could not generate CLIP query embedding. Skipping reranking.\")\n",
        "            return initial_yt_results\n",
        "\n",
        "        # 1. Perform a large search on the CLIP FAISS index\n",
        "        # k_clip_search determines how many top frame embeddings to retrieve from CLIP index\n",
        "        # This should be large enough to cover potential matches for videos in initial_yt_results\n",
        "        num_total_clip_embeddings = clip_idx.ntotal\n",
        "        k_clip_search = min(num_total_clip_embeddings, max(500, k_for_rerank * 10)) # Heuristic\n",
        "\n",
        "        print(f\"Searching CLIP FAISS index (top {k_clip_search} frames)...\")\n",
        "        distances_clip, indices_clip = clip_idx.search(clip_query_embedding, k_clip_search)\n",
        "\n",
        "        # 2. Create a mapping from video_id to its best CLIP score found in the search\n",
        "        video_id_to_best_clip_score = {}\n",
        "        if indices_clip.size > 0: # Check if any results were returned\n",
        "            for i in range(indices_clip.shape[1]):\n",
        "                embedding_idx = indices_clip[0][i]\n",
        "                score = distances_clip[0][i] # For IndexFlatIP, this is the dot product (similarity)\n",
        "\n",
        "                if 0 <= embedding_idx < len(clip_meta):\n",
        "                    video_id = clip_meta[embedding_idx].get('video_id')\n",
        "                    if video_id:\n",
        "                        # If video_id already has a score, keep the higher one (max similarity)\n",
        "                        if video_id not in video_id_to_best_clip_score or score > video_id_to_best_clip_score[video_id]:\n",
        "                            video_id_to_best_clip_score[video_id] = float(score)\n",
        "                else:\n",
        "                    print(f\"Warning: embedding_idx {embedding_idx} out of bounds for clip_meta (len {len(clip_meta)}).\")\n",
        "\n",
        "        print(f\"Found CLIP scores for {len(video_id_to_best_clip_score)} unique video IDs.\")\n",
        "\n",
        "        # 3. Separate initial results into those found in CLIP search and those not found\n",
        "        results_to_rerank_with_clip_score = []\n",
        "        results_not_in_clip_search = []\n",
        "\n",
        "        for item in initial_yt_results:\n",
        "            item_video_id = item.get('video_id')\n",
        "            if item_video_id in video_id_to_best_clip_score:\n",
        "                item_with_score = item.copy() # Avoid modifying original list items directly\n",
        "                item_with_score['clip_score'] = video_id_to_best_clip_score[item_video_id]\n",
        "                results_to_rerank_with_clip_score.append(item_with_score)\n",
        "            else:\n",
        "                results_not_in_clip_search.append(item)\n",
        "\n",
        "        # 4. Sort the rerankable results by their new 'clip_score' in descending order\n",
        "        if results_to_rerank_with_clip_score:\n",
        "            results_to_rerank_with_clip_score.sort(key=lambda x: x['clip_score'], reverse=True)\n",
        "            # Optionally remove the 'clip_score' if not needed in the final output\n",
        "            # for item in results_to_rerank_with_clip_score: item.pop('clip_score', None)\n",
        "            print(f\"Reranked {len(results_to_rerank_with_clip_score)} videos using CLIP scores.\")\n",
        "\n",
        "        # 5. Combine reranked results with those not found in CLIP search\n",
        "        final_reranked_results = results_to_rerank_with_clip_score + results_not_in_clip_search\n",
        "\n",
        "        print(\"CLIP reranking process completed.\")\n",
        "        return final_reranked_results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during CLIP reranking for query '{query_text}': {e}\")\n",
        "        # import traceback\n",
        "        # traceback.print_exc()\n",
        "        return initial_yt_results # Fallback to original results in case of error\n",
        "\n",
        "# --- Flask App Setup (from service script) ---\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    global ngrok_public_url\n",
        "    if ngrok_public_url:\n",
        "        return f\"\"\"\n",
        "        <h1>Combined QA & YouTube FAISS Search Service</h1>\n",
        "        <p>Service is running and accessible via ngrok.</p>\n",
        "        <p>Use the /search endpoint with a 'query' parameter and optionally a 'k' parameter.</p>\n",
        "        <p><b>Example:</b> <a href=\"{ngrok_public_url}/search?query=Explain+photosynthesis&k=3\" target=\"_blank\">{ngrok_public_url}/search?query=Explain+photosynthesis&k=3</a></p>\n",
        "        <p>Current ngrok URL: {ngrok_public_url}</p>\n",
        "        \"\"\"\n",
        "    else:\n",
        "        return \"<h1>Combined QA & YouTube FAISS Search Service</h1><p>Service is starting, ngrok URL not yet available...</p>\"\n",
        "\n",
        "@app.route(\"/search\", methods=[\"GET\"])\n",
        "def search_endpoint():\n",
        "    query_text = request.args.get(\"query\")\n",
        "    try:\n",
        "        k_results = int(request.args.get(\"k\", default=5))\n",
        "        if k_results <= 0: k_results = 5\n",
        "    except ValueError:\n",
        "        k_results = 5\n",
        "\n",
        "    if not query_text:\n",
        "        return jsonify({\"error\": \"Query parameter 'query' is required.\"}), 400\n",
        "\n",
        "    print(f\"Received search request: Query='{query_text}', k={k_results}\")\n",
        "    try:\n",
        "        query_vec = embed_query(query_text, query_embedding_pipeline_model, spark)\n",
        "        if query_vec is None: # Should not happen if embed_query raises error, but defensive\n",
        "            return jsonify({\"error\": f\"Could not generate embedding for query: '{query_text}'\"}), 500\n",
        "\n",
        "        qa_search_results = search_qa_faiss_service(faiss_index_qa, query_vec, k_results, metadata_pd_df_qa)\n",
        "        yt_search_results_initial = search_yt_faiss_service(faiss_index_yt, query_vec, k_results, metadata_pd_df_yt)\n",
        "\n",
        "        # --- Apply CLIP-based reranking to YouTube results ---\n",
        "        if faiss_index_clip and metadata_clip and clip_model and clip_processor:\n",
        "            print(f\"Attempting CLIP reranking for YouTube results (initial count: {len(yt_search_results_initial)}).\")\n",
        "            yt_search_results_reranked = rerank_youtube_results_with_clip(\n",
        "                initial_yt_results=yt_search_results_initial,\n",
        "                query_text=query_text,\n",
        "                k_for_rerank=k_results, # Pass k_results to help determine CLIP search depth\n",
        "                clip_m=clip_model,\n",
        "                clip_p=clip_processor,\n",
        "                clip_idx=faiss_index_clip,\n",
        "                clip_meta=metadata_clip,\n",
        "                device=clip_device\n",
        "            )\n",
        "            final_yt_results = yt_search_results_reranked\n",
        "        else:\n",
        "            print(\"CLIP resources not available. Using original YouTube search results.\")\n",
        "            final_yt_results = yt_search_results_initial\n",
        "        # --- End of CLIP reranking ---\n",
        "\n",
        "        response_data = {\n",
        "            \"qa_results\": qa_search_results,\n",
        "            \"yt_results\": final_yt_results # Use the potentially reranked results\n",
        "        }\n",
        "        print(f\"Search for '{query_text}' (k={k_results}) completed. Found {len(qa_search_results)} QA results, {len(final_yt_results)} YT results.\")\n",
        "        return jsonify(response_data)\n",
        "\n",
        "    except RuntimeError as e: # Catch error from embed_query if model not ready\n",
        "        print(f\"RuntimeError during search: {e}\")\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "    except Exception as e:\n",
        "        print(f\"Unexpected error during search for '{query_text}': {e}\")\n",
        "        # Log the full traceback for debugging if possible in a real server\n",
        "        # import traceback; traceback.print_exc()\n",
        "        return jsonify({\"error\": \"An unexpected error occurred during search.\"}), 500\n",
        "\n",
        "def run_flask_app():\n",
        "    log = logging.getLogger('werkzeug')\n",
        "    log.setLevel(logging.ERROR)\n",
        "    app.run(host=\"0.0.0.0\", port=5000, debug=False, use_reloader=False)\n",
        "\n",
        "# --- Main Execution Logic (from service script) ---\n",
        "if __name__ == \"__main__\":\n",
        "    # Initializations are done above. Critical checks ensure components are ready.\n",
        "    print(\"\\n--- Setting up ngrok tunnel ---\")\n",
        "    ngrok_auth_token = os.environ.get(\"NGROK_AUTHTOKEN\")\n",
        "    if ngrok_auth_token:\n",
        "        ngrok.set_auth_token(ngrok_auth_token)\n",
        "        print(\"NGROK Authtoken set.\")\n",
        "    else:\n",
        "        print(\"NGROK_AUTHTOKEN not found. Using ngrok without token (might have limitations).\")\n",
        "\n",
        "    try:\n",
        "        public_url_obj = ngrok.connect(5000, hostname=\"novel-osprey-uncommon.ngrok-free.app\")\n",
        "        ngrok_public_url = public_url_obj.public_url\n",
        "        print(f' * ngrok tunnel \"{ngrok_public_url}\" -> \"http://127.0.0.1:5000\"')\n",
        "        print(f\" * Access the service at: {ngrok_public_url}\")\n",
        "        print(f\" * Example search: {ngrok_public_url}/search?query=your_query_here&k=3\")\n",
        "\n",
        "        print(\"\\n--- Starting Flask App in a background thread ---\")\n",
        "        flask_thread = threading.Thread(target=run_flask_app, daemon=True)\n",
        "        flask_thread.start()\n",
        "        print(\"Flask app is running. Colab cell will remain active.\")\n",
        "        print(\"To stop: interrupt/stop the Colab kernel.\")\n",
        "\n",
        "        while flask_thread.is_alive():\n",
        "            time.sleep(1)\n",
        "\n",
        "    except KeyboardInterrupt:\n",
        "        print(\"\\nShutdown signal received.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during ngrok/Flask main loop: {e}\")\n",
        "    finally:\n",
        "        print(\"\\n--- Shutting down ---\")\n",
        "        if ngrok:\n",
        "            print(\"Closing ngrok tunnels...\")\n",
        "            try:\n",
        "                tunnels = ngrok.get_tunnels()\n",
        "                for tunnel in tunnels:\n",
        "                    ngrok.disconnect(tunnel.public_url)\n",
        "                ngrok.kill()\n",
        "                print(\"ngrok tunnels closed.\")\n",
        "            except Exception as ng_e:\n",
        "                print(f\"Error closing ngrok: {ng_e}\")\n",
        "        if spark and spark.getActiveSession():\n",
        "             print(\"Closing Spark session...\")\n",
        "             spark.stop()\n",
        "             print(\"Spark session closed.\")\n",
        "        print(\"Script execution finished.\")\n",
        "\n",
        "# Removed: print(search(\"how does single cell RNA seq work?\")) - now handled by API\n",
        "# Removed: import atexit - spark.stop() is in finally block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIe2jf9-iS5a"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
